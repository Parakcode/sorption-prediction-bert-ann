import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn.neural_network import MLPClassifier
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.metrics import r2_score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
import io

url='/content/4.csv'
df = pd.read_csv(url)
df.dropna()
y=df['LOG(KOC)']
X=df.loc[:,df.columns!='LOG(KOC)']
# decision tree for feature importance on a regression problem
from sklearn.datasets import make_regression
from sklearn.tree import DecisionTreeRegressor
from matplotlib import pyplot


from sklearn.feature_selection import SelectKBest, f_classif
#Suppose, we select 5 features with top 5 Fisher scores
selector = SelectKBest(f_classif, k = 40)
#New dataframe with the selected features for later use in the classifier. fit() method works too, if you want only the feature names and their corresponding scores
X_new = selector.fit_transform(X, y)
names = X.columns.values[selector.get_support()]
scores = selector.scores_[selector.get_support()]
names_scores = list(zip(names, scores))
ns_df = pd.DataFrame(data = names_scores, columns=['Feat_names', 'F_Scores'])
#Sort the dataframe for better visualization
ns_df_sorted = ns_df.sort_values(['F_Scores', 'Feat_names'], ascending = [False, True])
print(ns_df_sorted)
from pandas.core.frame import validate_axis_style_args
v=X.columns.values[selector.get_support()]
print(v)
url='/content/4.csv'
df = pd.read_csv(url)
df.dropna()
df.describe().transpose()
target_column = df[['LOG(KOC)']]
predictors = df[v]
from matplotlib.pyplot import figure

figure(figsize=(13,5))
plt.bar(v,scores, width=0.8)
plt.xlabel('Descriptors')
plt.ylabel('Importance')
plt.title('Highest Scored Descriptors ')
plt.xticks(rotation=90)

from sklearn.feature_selection import SelectKBest, f_classif
#Suppose, we select 5 features with top 5 Fisher scores
selector = SelectKBest(f_classif, k = 40)
#New dataframe with the selected features for later use in the classifier. fit() method works too, if you want only the feature names and their corresponding scores
X_new = selector.fit_transform(X, y)
names = X.columns.values[selector.get_support()]
scores = selector.scores_[selector.get_support()]
names_scores = list(zip(names, scores))
ns_df = pd.DataFrame(data = names_scores, columns=['Feat_names', 'F_Scores'])
#Sort the dataframe for better visualization
ns_df_sorted = ns_df.sort_values(['F_Scores', 'Feat_names'], ascending = [False, True])
print(ns_df_sorted)
from pandas.core.frame import validate_axis_style_args
v=X.columns.values[selector.get_support()]
print(v)
url='/content/4.csv'
df = pd.read_csv(url)
df.dropna()
df.describe().transpose()
target_column = df[['LOG(KOC)']]
predictors = df[v]
from matplotlib.pyplot import figure

figure(figsize=(13,5))
plt.bar(v,scores, width=0.8)
plt.xlabel('Descriptors')
plt.ylabel('Importance')
plt.title('Highest Scored Descriptors ')
plt.xticks(rotation=90)


X = predictors.values
y = np.asarray(target_column, dtype="|S6")
### Sandardization of data ###
from sklearn.preprocessing import StandardScaler
PredictorScaler=StandardScaler()
TargetVarScaler=StandardScaler()

# Storing the fit object for later reference
PredictorScalerFit=PredictorScaler.fit(X)
TargetVarScalerFit=TargetVarScaler.fit(y)

# Generating the standardized values of X and y
X=PredictorScalerFit.transform(X)
y=TargetVarScalerFit.transform(y)

# Split the data into training and testing set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Quick sanity check with the shapes of Training and testing datasets
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

from sklearn.svm import SVR
svr_rbf = SVR(kernel='rbf',gamma='scale', C=13, epsilon=0.1)
svr_rbf.fit(X_train, y_train)
from sklearn.model_selection import GridSearchCV
params = {'C':np.arange(0,20,0.2),'epsilon':np.arange(0,1,0.2)}
grid = GridSearchCV(svr_rbf,param_grid=params,cv=5,scoring='r2',verbose=1,return_train_score=True)
grid.fit(X_train,y_train)
grid.best_estimator_

import matplotlib.pyplot as plt
import numpy as np
from sklearn.svm import SVR
import math
svr_best=SVR(kernel='rbf',gamma='auto', C= 9.6, epsilon=0.2)
svr_best.fit(X_train, y_train)

y_pred = svr_best.predict(X_test)

# Calculate R^2 score
r2 = r2_score(y_test,y_pred)

# Calculate RMSE score
rmse = math.sqrt(mean_squared_error(y_test,y_pred))

print("R^2 Score: ", r2)
print("RMSE Score: ", rmse)

error = y_test - y_pred

plt.hist(error, bins=25)
plt.xlabel('Prediction Error')
plt.ylabel('Count')
plt.title('Error Histogram(test set)')
plt.show()


# Calculate the mean and standard deviation of the errors
mean = np.mean(error)
std_dev = np.std(error)

# Print the mean and standard deviation values
print("Mean error:", mean)
print("Standard deviation of error:", std_dev)
