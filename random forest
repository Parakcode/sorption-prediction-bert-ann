import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn.neural_network import MLPClassifier
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.metrics import r2_score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
import io

url='/content/4.csv'
df = pd.read_csv(url)
df.dropna()
y=df['LOG(KOC)']
X=df.loc[:,df.columns!='LOG(KOC)']
# decision tree for feature importance on a regression problem
from sklearn.datasets import make_regression
from sklearn.tree import DecisionTreeRegressor
from matplotlib import pyplot

from __future__ import print_function
import numpy as num
from sklearn import datasets, linear_model
from sklearn.feature_selection import SelectKBest, f_classif
import pandas as pd
import numpy as np
#Suppose, we select 5 features with top 5 Fisher scores
selector = SelectKBest(f_classif, k = 30)
#New dataframe with the selected features for later use in the classifier. fit() method works too, if you want only the feature names and their corresponding scores
X_new = selector.fit_transform(X, y)
names = X.columns.values[selector.get_support()]
scores = selector.scores_[selector.get_support()]
names_scores = list(zip(names, scores))
ns_df = pd.DataFrame(data = names_scores, columns=['Feat_names', 'F_Scores'])
#Sort the dataframe for better visualization
ns_df_sorted = ns_df.sort_values(['F_Scores', 'Feat_names'], ascending = [False, True])
print(ns_df_sorted)
from pandas.core.frame import validate_axis_style_args
v=X.columns.values[selector.get_support()]
print(v)
url='/content/4.csv'
df = pd.read_csv(url)
df.dropna()
df.describe().transpose()
target_column = df[['LOG(KOC)']]
predictors = df[v]
X = predictors.values
y = np.asarray(target_column, dtype="|S6")
### Sandardization of data ###
from sklearn.preprocessing import StandardScaler
PredictorScaler=StandardScaler()
TargetVarScaler=StandardScaler()

# Storing the fit object for later reference
PredictorScalerFit=PredictorScaler.fit(X)
TargetVarScalerFit=TargetVarScaler.fit(y)

# Generating the standardized values of X and y
X=PredictorScalerFit.transform(X)
y=TargetVarScalerFit.transform(y)

# Split the data into training and testing set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Quick sanity check with the shapes of Training and testing datasets
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

from sklearn.metrics import mean_squared_error
import math
# Fit Regression Model
reg = RandomForestRegressor(n_estimators=200, warm_start=True, min_samples_split=2, min_samples_leaf=1, max_depth=20, max_features=4)
reg.fit(X_train,y_train)
from sklearn.model_selection import GridSearchCV
params = {'n_estimators':np.arange(100,300,5),'max_depth':np.arange(1,50,4)}
grid = GridSearchCV(reg,param_grid=params,cv=5,scoring='r2',verbose=1,return_train_score=True)
grid.fit(X_train,y_train)
reg = RandomForestRegressor(n_estimators=215, warm_start=True, min_samples_split=2, min_samples_leaf=1, max_depth=29, max_features=4)
reg.fit(X_train,y_train)
# Calculate Training and Test Accuracy
training_accuracy = reg.score(X_train, y_train)
test_accuracy = reg.score(X_test, y_test)

# Calculate Root mean squared error
rmse_train = np.sqrt(mean_squared_error(reg.predict(X_train),y_train))
rmse_test = np.sqrt(mean_squared_error(reg.predict(X_test),y_test))
print("Training Accuracy = %0.3f, Test Accuracy = %0.3f, RMSE (train) = %0.3f, RMSE (test) = %0.3f" % (training_accuracy, test_accuracy, rmse_train, rmse_test))
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor

reg = RandomForestRegressor(n_estimators=120, warm_start=True, min_samples_split=2, min_samples_leaf=1, max_depth=33, max_features=4)
reg.fit(X_train,y_train)

y_pred = reg.predict(X_test)

# Calculate R^2 score
r2 = r2_score(y_test,y_pred)

# Calculate RMSE score
rmse = math.sqrt(mean_squared_error(y_test,y_pred))

print("R^2 Score: ", r2)
print("RMSE Score: ", rmse)
import seaborn as sns

sns.regplot(x=y_pred, y=y_test)


import numpy as np
import matplotlib.pyplot as plt

m, b = np.polyfit(np.squeeze(y_test), np.squeeze(y_pred), 1)
plt.scatter(np.squeeze(y_test), np.squeeze(y_pred), label='Data')
plt.plot(np.squeeze(y_test), m*np.squeeze(y_test) + b,label='Fit')
plt.xlabel('Experimental log(Koc)')
plt.ylabel('Predicted log(Koc)')
plt.text(0.5, 0.9, 'y = {}x + {}'.format(m, b), fontsize=12,
         transform=plt.gcf().transFigure)
plt.title('test set', loc='left')
plt.legend()
plt.show()
import matplotlib.pyplot as plt
import numpy as np
from sklearn.svm import SVR
import math
svr_best=SVR(kernel='rbf',gamma='auto', C= 9.6, epsilon=0.2)
svr_best.fit(X_train, y_train)

y_pred = svr_best.predict(X_test)

# Calculate R^2 score
r2 = r2_score(y_test,y_pred)

# Calculate RMSE score
rmse = math.sqrt(mean_squared_error(y_test,y_pred))

print("R^2 Score: ", r2)
print("RMSE Score: ", rmse)

error = y_test - y_pred

plt.hist(error, bins=25)
plt.xlabel('Prediction Error')
plt.ylabel('Count')
plt.title('Error Histogram(test set)')
plt.show()


# Calculate the mean and standard deviation of the errors
mean = np.mean(error)
std_dev = np.std(error)

# Print the mean and standard deviation values
print("Mean error:", mean)
print("Standard deviation of error:", std_dev)
y_pred = reg.predict(X_train)

# Calculate R^2 score
r2 = r2_score(y_train,y_pred)

# Calculate RMSE score
rmse = math.sqrt(mean_squared_error(y_train,y_pred))

print("R^2 Score: ", r2)
print("RMSE Score: ", rmse)
import seaborn as sns

sns.regplot(x=y_pred, y=y_train)


import numpy as np
import matplotlib.pyplot as plt

m, b = np.polyfit(np.squeeze(y_train), np.squeeze(y_pred), 1)
plt.scatter(np.squeeze(y_train), np.squeeze(y_pred), label='Data')
plt.plot(np.squeeze(y_train), m*np.squeeze(y_train) + b,label='Fit')
plt.xlabel('Experimental log(Koc)')
plt.ylabel('Predicted log(Koc)')
plt.text(0.5, 0.9, 'y = {}x + {}'.format(m, b), fontsize=12,
         transform=plt.gcf().transFigure)
plt.title('test set', loc='left')
plt.legend()
plt.show()
